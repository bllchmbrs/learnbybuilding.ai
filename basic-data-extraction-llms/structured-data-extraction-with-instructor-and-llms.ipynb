{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aeee00d-c89e-4e9c-aadd-619dfcf7dc96",
   "metadata": {},
   "source": [
    "# Structured Data Extraction using LLMs and Instructor\n",
    "\n",
    "In this post, we're going to be extracting structured data from a podcast transcript. We've all seen the ability for generative models to be effective *generating* text.\n",
    "\n",
    "**But what about extracting data?**\n",
    "\n",
    "Data extraction is a far more common use case (today) than generation, in particular for businesses. Businesses have to process all kinds of documents, exchange them and so on.\n",
    "\n",
    "**Emerging Use Case: LLMs for data extraction**\n",
    "\n",
    "If you think about it, summarization is effectively data extraction. We've seen LLMs perform well at summarization, but did you know that they can extract structured data quite well?\n",
    "\n",
    "## What you'll learn from this post\n",
    "\n",
    "In this post, you'll learn how to extract structured data from LLMs into Pydantic objects using the [Instructor](https://jxnl.github.io/instructor/) library.\n",
    "\n",
    "## Here's us setting up on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eae8c64-ee1c-4990-b8f7-13b052844cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3eabca-c34f-4027-85e8-6470c64bda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructor==0.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14884fa-50ea-4c8c-bacc-0782a9b6ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.2\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "print(pydantic.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee2405c-8a21-41b1-b38d-63f242f3fd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf851-e19b-4989-b138-0846ea7583ab",
   "metadata": {},
   "source": [
    "## Background on Instructor\n",
    "\n",
    "[Instructor](https://jxnl.github.io/instructor/) is a library that helps get structured data out of LLMs. It has narrow dependencies and a simple API, requiring basic nothing sophisticated from the end user.\n",
    "\n",
    "In the author's words:\n",
    "\n",
    "> `Instructor` helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the `Pydantic` model for your desired response, `Instructor` handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.\n",
    "\n",
    "The library is still early, version 0.2.9 has a modest star count (1300) but hits above it's weight since it's basically a single person working on it.\n",
    "\n",
    "![instructor contributors page](https://images.learnbybuilding.ai/instructor-contributions-page.webp)\n",
    "\n",
    "Now just because it's one person doesn't mean it's bad. In fact, I found it to be very simple to use. It just means that from a long term maintenance perspective, it might be challenging. Something to note.\n",
    "\n",
    "## Pre-processing the data\n",
    "\n",
    "Here, we are fetching and parsing an RSS feed from a podcast. We specifically target an episode by its title and then retrieve its summary from the podcast description. This is similar to a [tutorial on llamaindex](https://learnbybuilding.ai/tutorials/rag-chatbot-on-podcast-llamaindex-faiss-openai) that we recently published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6d064d-0f75-4b9e-a610-955ea43e1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><em>Thanks to the </em><em>over 11,000 people</em><em> who joined us for the first AI Engineer Su\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "podcast_atom_link = \"https://api.substack.com/feed/podcast/1084089.rss\" # latent space podcastbbbbb\n",
    "parsed = feedparser.parse(podcast_atom_link)\n",
    "episode = [ep for ep in parsed.entries if ep['title'] == \"Why AI Agents Don't Work (yet) - with Kanjun Qiu of Imbue\"][0]\n",
    "episode_summary = episode['summary']\n",
    "print(episode_summary[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb7b8-c1fd-410e-a5cf-8bb08cb6235b",
   "metadata": {},
   "source": [
    "Now we're going to extract the shortened episode summary. We'll also shorten the summary just to speed up our extraction. In the future, we can extend the transcript to cover the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2af4fc2-6085-45b4-935f-e67f71f24450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of the transcript: 58\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "parsed_summary = partition_html(text=''.join(episode_summary)) \n",
    "start_of_transcript = [x.text for x in parsed_summary].index(\"Transcript\") + 1\n",
    "print(f\"First line of the transcript: {start_of_transcript}\")\n",
    "text = '\\n'.join(t.text for t in parsed_summary[start_of_transcript:])\n",
    "text = text[:3508] # shortening the transcript for speed & cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0efc7-1cfe-4733-be98-40b34b11189b",
   "metadata": {},
   "source": [
    "## Using Instructor\n",
    "\n",
    "Instructor is the \"thinnest\" of [the libraries that I've used for structured data extraction](https://learnbybuilding.ai/vs/marvin-ai-vs-guardrails-vs-instructor). The interface is super simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff585fc-17f2-480c-81e3-f2123676a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    school: Optional[str] = Field(..., description=\"The school this person attended\")\n",
    "    company: Optional[str] = Field(..., description=\"The company this person works for \")\n",
    "\n",
    "class People(BaseModel):\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821e718-f11c-40ec-9236-532a8c1790b1",
   "metadata": {},
   "source": [
    "All we do is [Monkey patch](https://en.wikipedia.org/wiki/Monkey_patch#:~:text=Monkey%20patching%20is%20a%20technique,Python%2C%20Groovy%2C%20etc.)) OpenAI's SDK.\n",
    "\n",
    "**Note:** This could be hard to maintain, so take note of versions and be sure to not let either one get upgraded without testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a3949e-e276-4576-8eaf-38fc97af5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import instructor\n",
    "\n",
    "instructor.patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb2d83-edd2-4411-a08d-473657633f1e",
   "metadata": {},
   "source": [
    "Now we simply call OpenAI but ask for a `response_model`. This is the instructor patch at work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2523c901-a899-4c64-b04b-8bbd210a1cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Alessio', school=None, company='Decibel Partners'), Person(name='Swyx', school=None, company='Smol.ai'), Person(name='Kanjun', school='MIT', company='Imbue'), Person(name='Josh', school=None, company='Imbue')]\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=People,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335844c-f84b-42c5-af8a-fa32652ccc06",
   "metadata": {},
   "source": [
    "Overall the result is high quality, we've gotten all the names and companies.\n",
    "\n",
    "Now we can take this to the next level by trying to extract multiple objects at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "196ab8ea-d16f-40bd-b243-859440588790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Alessio', school=None, company='Decibel Partners'), Person(name='Swyx', school=None, company='Smol.ai'), Person(name='Kanjun', school='MIT', company='Imbue'), Person(name='Josh', school=None, company='Ember')] companies=[Company(name='Decibel Partners'), Company(name='Smol.ai'), Company(name='Imbue'), Company(name='Generally Intelligent'), Company(name='Ember'), Company(name='Sorceress'), Company(name='Dropbox'), Company(name='MIT Media Lab'), Company(name='OpenA')] research_papers=None\n"
     ]
    }
   ],
   "source": [
    "class Company(BaseModel):\n",
    "    name:str\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    paper_name:str = Field(..., description=\"an academic paper reference discussed\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    people: List[Person]\n",
    "    companies: List[Company]\n",
    "    research_papers: Optional[List[ResearchPaper]]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=ExtractedInfo,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484bb8-343e-4696-8867-bc9416fe5c6a",
   "metadata": {},
   "source": [
    "You can see how well this works, to grab a whole bunch of structured data for us with basically no work. Right now we're just working on a excerpt of this data, but with basically zero NLP specific work, we're able to get some pretty powerful results.\n",
    "\n",
    "\n",
    "## Wrapping it all up\n",
    "\n",
    "The sky is really the limit here. There's so much structured data that's been locked up in unstructured text. I'm bullish on this space and can't wait to see what you build with this tool!\n",
    "\n",
    "If you found this interesting, you might want to see similar posts on:\n",
    "1. [Comparing 3 Data Extraction Libraries: Marvin, Instructor, and Guardrails](https://learnbybuilding.ai/vs/marvin-ai-vs-guardrails-vs-instructor)\n",
    "2. [Structured Data Extraction using LLMs and Marvin](https://learnbybuilding.ai/tutorials/structured-data-extraction-with-marvin-ai-and-llms)\n",
    "3. [Structured Data Extraction using LLMs and Guardrails AI](https://learnbybuilding.ai/tutorials/structured-data-extraction-with-guardrails-and-llms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
