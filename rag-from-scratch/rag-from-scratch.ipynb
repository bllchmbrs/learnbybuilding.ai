{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594d84d5-c684-45c3-a6b5-e797c6d628c8",
   "metadata": {},
   "source": [
    "# Building a RAG Application from Scratch\n",
    "\n",
    "All these vendors are trying to overcomplicate retrieval augmented generation (RAG). They're trying to inject different tools all over the place and make it more complicated than it needs to be. Let's change that.\n",
    "\n",
    "This tutorial is going to teach you how to build RAG applications from scratch. No fluff, no jargon, no libraries, just a simple step by step RAG application.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## The High Level Components of a RAG System\n",
    "\n",
    "1. a collection of documents (formally called a corpus)\n",
    "2. An input from the user\n",
    "3. a similarity measure between the collection of documents and the user input\n",
    "\n",
    "Yes, it's that simple. You don't need a vector store, you don't even *need* an LLM.\n",
    "\n",
    "## The ordered steps of a RAG system\n",
    "\n",
    "We'll perform the following steps in sequence.\n",
    "\n",
    "1. Receive a user input\n",
    "2. Perform our similarity measure.\n",
    "3. Post-process the user input and the fetched document(s)\n",
    "\n",
    "\n",
    "## Working through an example - the simplest RAG system\n",
    "\n",
    "Let's walk through all of this with code examples.\n",
    "\n",
    "### Getting a collection of documents\n",
    "\n",
    "Below you can see that we've got a simple corpus of 'documents' (please be generous ðŸ˜‰)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f357b0d9-0142-479b-bd89-a5d889e314e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_documents = [\n",
    "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
    "    \"Visit a local museum and discover something new.\",\n",
    "    \"Attend a live music concert and feel the rhythm.\",\n",
    "    \"Go for a hike and admire the natural scenery.\",\n",
    "    \"Have a picnic with friends and share some laughs.\",\n",
    "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
    "    \"Take a yoga class and stretch your body and mind.\",\n",
    "    \"Join a local sports league and enjoy some friendly competition.\",\n",
    "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
    "    \"Visit an amusement park and ride the roller coasters.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfb9ed-34cd-4f9f-b3b4-e592cbc2a8d5",
   "metadata": {},
   "source": [
    "### Defining and performing the similarity measure\n",
    "\n",
    "Now we need a way of measuring the similarity between the **user input** we're going to receive and the **collection** of documents that we organized. Arguably the simplest similarity measure is [jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index). I've written about that in the past (see [this post](https://billchambers.me/posts/tf-idf-explained-in-python) but the short answer is that the **jaccard similarity** is the intersection divided by the union of the \"sets\" of words.\n",
    "\n",
    "This allows us to compare our user input with the source documents.\n",
    "\n",
    "#### Side note: preprocessing\n",
    "\n",
    "A challenge is that if we have a plain string like `\"Take a leisurely walk in the park and enjoy the fresh air.\",`, we're going to have to pre-process that into a set, so that we can perform these comparisons. We're going to do this in the simplest way possible, lower case and split by `\" \"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbe933b4-b1df-403b-9e56-8a24bcc9896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366758a-81a1-4e4b-aedc-49d8dc79d951",
   "metadata": {},
   "source": [
    "Now we need to define a function that takes in the exact query and our corpus and selects the 'best' document to return to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af9d6da1-ff8d-4107-9b79-145aaef86baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_response(query, corpus):\n",
    "    similarities = []\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(user_input, doc)\n",
    "        similarities.append(similarity)\n",
    "    return corpus_of_documents[similarities.index(max(similarities))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cfd37-8024-427c-9d82-7de05fdc58a7",
   "metadata": {},
   "source": [
    "Now we can run it, we'll start with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28e22068-9d08-4d59-938c-b63d9459491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is a leisure activity that you like?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e10538-27b2-4fc8-8b35-9f49d4fe5e73",
   "metadata": {},
   "source": [
    "And a simple user input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f92afe2-03f9-41d5-8259-7bed04007785",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I like to hike\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0343ae-c479-4a3d-866c-9a614e10f122",
   "metadata": {},
   "source": [
    "Now we can return our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba966156-8a47-4840-893e-2f2be8b15dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go for a hike and admire the natural scenery.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_response(user_input, corpus_of_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83bd30-b6f1-4b1d-bdee-b393ad31132d",
   "metadata": {},
   "source": [
    "Congrats, you've built a basic RAG application.\n",
    "\n",
    "\n",
    "#### I got 99 problems and bad similarity is one\n",
    "\n",
    "Now we've opted for a simple similarity measure for learning. But this is going to be problematic because it's so simple. It has no notion of **semantics**. It's just looks at what words are in both documents. That means that if we provide a negative example, we're going to get the same \"result\" because that's the closest document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eedd2c0e-db92-4a87-93f7-538d211b6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I don't like to hike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10d9b378-e060-4ceb-ae56-634f4f64d742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go for a hike and admire the natural scenery.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_response(user_input, corpus_of_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa1017-7888-44fe-b5d4-650445559a2b",
   "metadata": {},
   "source": [
    "This is a topic that's going to come up a lot with \"RAG\", but for now, rest assured that we'll address this problem later.\n",
    "\n",
    "At this point, we've done zero post processing of the \"document\" that we're responding to. So we've really only done the \"retrieval\" part of \"Retrieval-Augmented Generation\". Let's get to augmented generation by adding a large language model (LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0e3e9-ea5f-47c2-bcdb-ff18ef5fcd3e",
   "metadata": {},
   "source": [
    "## Adding in a LLM\n",
    "\n",
    "To do this, we're going to use [ollama](https://ollama.ai/) to get up and running with an open source LLM on our local machine. We could just as easily use OpenAI's gpt-4 or Anthropic's Claude but for now, we'll start with the open source llama2 from [Meta AI](https://ai.meta.com/llama/).\n",
    "\n",
    "- [ollama installation instructions are here](https://ollama.ai/)\n",
    "\n",
    "This post is going to assume some basic knowledge of large language models, so let's get right to querying this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3113f78d-86d3-4901-9800-76620fa71a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f2a88-f68b-433d-a803-425ca804ff2c",
   "metadata": {},
   "source": [
    "First we're going to define the inputs. To work with this model, we're going to take \n",
    "\n",
    "1. user input,\n",
    "2. fetch the most similar document (as measured by our similarity measure),\n",
    "3. pass that into a prompt to the language model,\n",
    "4. *then* return the result to the user\n",
    "\n",
    "That introduces a new term, the **prompt**. In short, it's the instructions that you provide to the LLM.\n",
    "\n",
    "When you run this code, you'll see the streaming result. Streaming is important for user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bce8d1c4-8cf9-4d56-8039-529db2b263b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_input = \"I like to hike\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "full_response = []\n",
    "\n",
    "# https://github.com/jmorganca/ollama/blob/main/docs/api.md\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful bot that makes recommendations for activities. You are helpful.\n",
    "\n",
    "This is the recommended activity: {relevant_document}\n",
    "\n",
    "The user input is: {user_input}\n",
    "\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f32073-e810-4735-9149-0048becc351c",
   "metadata": {},
   "source": [
    "Now that we've defined that, let's make the API call to ollama (and llama2).\n",
    "\n",
    "an important step is to make sure that ollama's running already on your local machine by running `ollama serve` (crazy, I know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5986ad22-3cc7-4e55-bddf-3bdab7d0041f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh, I see! Thank you for letting me know! Based on your input, I would recommend a different activity Thank you for considering my suggestions! I understand that hiking may not be everyone's cup of tea, so here are some alternative activities that you might enjoy:\n",
      "\n",
      "1. Visit a nearby park or nature reserve: Many cities have beautiful parks or nature reserves that offer a peaceful escape from the hustle and bustle of daily life. You can take a leisurely stroll, have a picnic, or simply sit and enjoy the surroundings.\n",
      "2. Go for a bike ride: If you're not interested in hiking, why not try going for a bike ride? Many cities have scenic bike routes that offer a fun and active way to explore the local area.\n",
      "3. Take a yoga or meditation class: Yoga and meditation can help you relax and unwind, and many studios offer classes specifically designed for beginners. This can be a great way to improve your mental and physical well-being without having to go for a hike.\n",
      "4. Visit a local museum or art gallery: If you're looking for something indoors, why not visit a local museum or art gallery? Many cities have world-class collections that offer a fascinating glimpse into the history and culture of the area.\n",
      "5. Take a cooking class: If you're interested in learning new skills, consider taking a cooking class. Many cooking schools offer classes for beginners, and this can be a fun and creative way to spend an afternoon.\n",
      "\n",
      "I hope these suggestions are helpful! Let me know if you have any other questions or preferences, and I'll do my best to provide more tailored recommendations.\n"
     ]
    }
   ],
   "source": [
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"llama2\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response']) # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fc761-ce9e-48f1-bdbd-213f841a8652",
   "metadata": {},
   "source": [
    "This gives us a complete RAG Application, from scratch, no providers, no services. You know all of the components in a Retrieval-Augmented Generation application.\n",
    "\n",
    "The LLM (if you're lucky) will handle the user input that goes against the recommended document. We can see that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "059d01cb-c4ed-4030-a014-7bdbaeca58a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m             decoded_line \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;66;03m# print(decoded_line['response']) # uncomment if you want to see the results token by token\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m             full_response\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdecoded_line\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     response\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response'"
     ]
    }
   ],
   "source": [
    "user_input = \"I don't like to hike\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "# https://github.com/jmorganca/ollama/blob/main/docs/api.md\n",
    "full_response = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful bot that makes recommendations for activities. You are helpful.\n",
    "\n",
    "This is the recommended activity: {relevant_document}\n",
    "\n",
    "The user input is: {user_input}\n",
    "\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"llama2\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08a0e0-3aaf-42af-a314-eeedb2a631a0",
   "metadata": {},
   "source": [
    "## Areas for improvement\n",
    "\n",
    "If we go back to our diagream of the RAG application and think about what we've just built, we'll see various opportunities for improvement. These opportunities are where tools like vector stores, embeddings, and prompt 'engineering' gets involved.\n",
    "\n",
    "Here are 10 potential areas for improvement:\n",
    "\n",
    "1. **The number of documents** ðŸ‘‰ more documents might mean more recommendations.\n",
    "2. **The depth/size of documents** ðŸ‘‰ higher quality content and longer documents with more information might be better.\n",
    "3. **The number of documents we give to the LLM** ðŸ‘‰ Right now, we're only giving the LLM one document. We could feed in several as 'context' and allow the model to provide a more personalized recommendation based on the user input.\n",
    "4. **The parts of documents that we give to the LLM** ðŸ‘‰ If we have bigger or more thorough documents, we might just want to add in parts of those documents, parts of various documents, or some variation there of. In the lexicon, this is called chunking.\n",
    "5. **Our document storage tool** ðŸ‘‰ We might store our documents in a different way or different database. In particular, if we have a lot of them, we might explore storing them in a data lake or a vector store.\n",
    "6. **The pre-processing of the documents & user input** ðŸ‘‰ We might perform some extra preprocessing or augmentation of the user input before we pass it into the similarity measure. For instance, we might use an embedding to convert that input to a vector.\n",
    "7. **The similarity measure** ðŸ‘‰ We can change the similarity measure to fetch better or more relevant documents.\n",
    "8. **The LLM/Model we use** ðŸ‘‰ We can change the final model that we use. We're using llama2 above, but we could just as easily use an Anthropic or Claude Model.\n",
    "9. **The prompt that we use** ðŸ‘‰ We could use a different prompt into the LLM/Model and tune it according to the output we want to get the output we want.\n",
    "10. **If you're worried about harmful or toxic output** ðŸ‘‰ We could implement a \"circuit breaker\" of sorts that runs the user input to see if there's toxic, harmful, or dangerous discussions. For instance, in a healthcare context you could see if the information contained suicidal or harm-to-others type languages and respond accordingly - outside of the typical flow.\n",
    "\n",
    "\n",
    "Now improvements don't stop here. They're quite limitless and that's what we'll get into in the future. Until then, [let me know if you have any questions on twitter](https://twitter.com/bllchmbrs) and happy RAGING :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71597c5b-5262-4eb9-8618-a37f07e94b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
